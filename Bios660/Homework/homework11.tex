\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{float}
\graphicspath{ {} }
\usepackage{mathtools}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{caption}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{Ty Darnell}
\lhead{Homework 11}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}
\allowdisplaybreaks
\begin{document}
\begin{flushleft}
\chead{Problems 1-3}
\section*{Problem 1}
\begin{align*}
\text{From} &\text{ Example 4.3.1 and Theorem 4.3.2 we know:}\\
X+Y &\sim \text{Poisson}(\theta+\lambda)\\
\text{Let } U&=X+Y, \ V=Y\\
\text{Then } X&=U-V, \ Y=V\\
f(u,v)&=\dfrac{\theta^{u-v}e^{-\theta}}{(u-v)!}\dfrac{\lambda^{v} e^{-\lambda}}{v!} \ v=0,1,\dots \ u=v,v+1, \dots\\
f(u)&=\dfrac{e^{-(\theta+\lambda)}}{u!}(\theta+\lambda)^u \ u=0,1,\dots\\
\text{Finding } &Y|X+Y\\
\text{Defining }& \text{U and V the same way:}\\
f(y|x+y)&= f(v|u)\\
f(v|u)&=\dfrac{f(u,v)}{f(u)}\\
&=\dfrac{\dfrac{\theta^{u-v}e^{-\theta}}{(u-v)!}\dfrac{\lambda^v e^{-\lambda}}{v!}}{\dfrac{e^{-(\theta+\lambda)}}{u!}(\theta+\lambda)^u}\\
&=\dfrac{u!}{(u-v)!v!}\dfrac{e^{-(\theta+\lambda)}}{e^{-(\theta+\lambda)}}\dfrac{\theta^{u-v}\lambda^v}{(\theta+\lambda)^u}\\
&={u\choose v}\dfrac{\theta^{u-v}\lambda^v}{(\theta+\lambda)^u}\\
&={u\choose v}\left(\dfrac{\lambda}{\theta+\lambda}\right)^{v}\left(\dfrac{\theta}{\theta+\lambda}\right)^{u-v}\\
\text{Which is }& \text{binomial}\left(u,\dfrac{\lambda}{\theta+\lambda}\right)\\
\text{Finding } &X|X+Y\\
\text{Define } U&=X+Y, \ V=X\\
\text{Then } X&=U-V, \ X=V\\
f_{U,V}(u,v)&=f_{X,Y}(v,u-v)=\dfrac{\theta^v e^{-\theta}}{v!} \dfrac{\lambda^{u-v} e^{-\lambda}}{(u-v)!}\\
f(u)&=\sum_{v=0}^{u}\dfrac{\theta^v e^{-\theta}}{v!}\dfrac{\lambda^{u-v}e^{-\lambda}}{(u-v)!}\\
&=\dfrac{e^{-(\theta+\lambda)}}{u!}\sum_{v=0}^{u}{u \choose v}\theta^v \lambda^{u-v}\\
\text{Using}&\text{ the binomial theorem we have:}\\
f(u)&=\dfrac{e^{-(\theta+\lambda)}}{u!}(\theta+\lambda)^u\\
f(x|x+y)&= f(v|u)\\
&=\dfrac{f(u,v)}{f(u)}\\
&=\dfrac{\dfrac{\theta^v e^{-\theta}}{v!} \dfrac{\lambda^{u-v} e^{-\lambda}}{(u-v)!}}{\dfrac{e^{-(\theta+\lambda)}}{u!}(\theta+\lambda)^u}\\
&={u \choose v}\dfrac{\theta^v \lambda^{u-v}}{(\theta+\lambda)^u}\\
&={u \choose v}\left(\dfrac{\theta}{\theta+\lambda}\right)^{v} \left(\dfrac{\lambda}{\theta+\lambda}\right)^{u-v}\\
\text{Which is }& \text{binomial}\left(u,\dfrac{\theta}{\theta+\lambda}\right)\\
\end{align*}
\section*{Problem 2}
$f_X(x)=p(1-p)^{x-1} \quad f_Y(y)=p(1-p)^{y-1}$\\
Since X and Y are independent we have:\\
$f_{X,Y}(x,y)=p(1-p)^{x-1}p(1-p)^{y-1}$\\
$=p^2(1-p)^{x+y-2}$\\
\begin{enumerate}[(a)]
\item 
Solving $V=X-Y$ for X we get $X=V+Y$\\ 
If $V>0$ then $X>Y$\\
Since $U=min(X,Y)$ this means that $U=Y$\\
Thus we have $Y=U$ and $X=U+V$
\begin{align*}
f_{U,V}(u,v)&=P(Y=u,X=u+v)\\
&=p^2(1-p)^{2u+v-2}\\
\text{Which factors to: }& (p^2(1-p)^{2u})((1-p)^{v-2})\\
\text{If } V&<0, \text{ then } X<Y\\
\text{Thus } X=U, \quad Y=U-V\\
f_{U,V}(u,v)&=P(X=u,Y=u-v)\\
&=p^2(1-p)^{2u-v-2}\\
\text{Which factors to: }& (p^2(1-p)^{2u})((1-p)^{-v-2})\\
\text{If } V&=0 \text{ then } X=Y\\
f_{U,V}(u,0)&=P(X=Y=u)=p^2(1-p)^{2u-2}\\
\text{Which factors to: }& (p^2(1-p)^{2u})((1-p)^{-2})\\
\text{Since we can factor}&\text{ all of these cases in terms of u and v, U and V are independent }
\end{align*}
\item 
\begin{align*}
Z&=\dfrac{X}{(X+Y)}\\
\text{Define } U&=X\\
Z&=r/s \text{ where } r<s\\
\dfrac{x}{x+y}&=r/s\\
\text{Then } X&=r \ Y=s-r\\
P(Z=r/s)&=\sum_{i=1}^{\infty}P(X=ir,Y=i(s-r))\\
&=\sum_{i=1}^{\infty}p(1-p)^{ir-1}p(1-p)^{i(s-r)-1}\\
&=\sum_{i=1}^{\infty}p^2(1-p)^{is-2}\\
&=\left(\dfrac{p}{1-p}\right)^2\sum_{i=1}^{\infty}((1-p)^s)^i\\
&=\left(\dfrac{p}{1-p}\right)^2 \dfrac{(1-p)^s}{1-(1-p)^s}\\
&=\dfrac{p^2(1-p)^{s-2}}{1-(1-p)^s}
\end{align*}
\item
\begin{align*}
\text{Define } T&=X+Y\\
f_{X,X+Y}(x,x+y)&=P(X=x,X+Y=t)=P(X=x,Y=t-x)=P(X=x)P(Y=t-x)\\
&=p^2(1-p)^{x-1+t-x-1}=p^2(1-p)^{t-2}
\end{align*}
\end{enumerate}
\section*{Problem 3}
\begin{enumerate}[(a)]
\item 
\begin{align*}
X_1,& X_2 \text{ are independent and distributed as:}\\
f_{X_i}(x_i)&=\dfrac{1}{\sqrt{2\pi}\sigma}e^{-x_i^2/2\sigma^2}\\
f_{X_1,X_2}(x_1,x_2)&=\dfrac{1}{2\pi\sigma^2}e^{-x_1^2/2\sigma^2}e^{-x_2^2/2\sigma^2}\\
&=\dfrac{1}{2\pi\sigma^2}e^{-(x_1^2+x_2^2)/2\sigma^2}\\
\text{Since the transformation}&\text{ is not one to one, we must partition the support of } (X_1,X_2)\\
A_0&=\{-\infty<x_1<\infty,x_2=0\}\\
A_1&=\{-\infty<x_1<\infty,x_2<0\}\\
A_2&=\{-\infty<x_1<\infty,x_2>0\}\\
\text{Since } & Y_2=\dfrac{X_1}{\sqrt{X_1^2+X_2^2}} \text{ it ranges from -1 to 1} \\
\text{The support of } & (Y_1, Y_2) \text{ is } \mathcal{B}=\{0<y_1<\infty, -1<y_2<1 \}\\
Y_1&=X_1^2+X_2^2 \quad Y_2=\dfrac{X_1}{\sqrt{Y_1}}\\
\text{For }& A_1:\\
X_1&=Y_2\sqrt{Y_1}\\
X_2&=\sqrt{Y_1-{Y_2}^2Y_1}\\
J_1&=\begin{bmatrix}
\dfrac{\partial x_1}{\partial y_1} & 
\dfrac{\partial x_1}{\partial y_2}\\ 
\dfrac{\partial x_2}{\partial y_1} &
\dfrac{\partial x_2}{\partial y_2} 
\end{bmatrix}
=\begin{bmatrix}
\dfrac{y_2}{2\sqrt{y_1}} & 
\sqrt{y_1}\\ 
\dfrac{\sqrt{1-y_2^2}}{2\sqrt{y_1}}&
\dfrac{\sqrt{y_1}y_2}{\sqrt{1-y_2^2}} 
\end{bmatrix}\\
&=\dfrac{1}{2\sqrt{1-y_2^2}}\\
\text{For } &A_2:\\
X_1&=Y_2\sqrt{Y_1}\\
X_2&=-\sqrt{Y_1-{Y_2}^2Y_1}\\
J_2&=-J_1\\
f_{Y_1,Y_2}(y_1,y_2)&=2\left(\dfrac{1}{2\pi\sigma^2}e^{-y_1/2\sigma^2}\right)\dfrac{1}{2\sqrt{1-y_2^2}}\\
&=\dfrac{1}{2\pi\sigma^2}e^{-y_1/2\sigma^2}\dfrac{1}{\sqrt{1-y_2^2}} \quad 0<y_1<\infty,-1<y_2<1
\end{align*}
\item 
\begin{align*}
f_{Y_1,Y_2}(y_1,y_2)&=\left(\dfrac{1}{2\pi\sigma^2}e^{-y_1/2\sigma^2}\right)\left(\dfrac{1}{\sqrt{1-y_2^2}}\right)
\end{align*}
Since the joint pdf factors into a function of $y_1$ and a function of $y_2$\\
$y_1$ and $y_2$ are independent.
\end{enumerate}
\pagebreak
\section*{Problem 4}
\chead{Problems 4-6}
\begin{enumerate}[(a)]
\item 
\begin{align*}
f_{X,Y}(x,y)&=\dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\dfrac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha+\beta)\Gamma(\gamma)}y^{\alpha+\beta-1}(1-y)^{\gamma-1}\\
0<&x<1 \quad 0<y<1\\
U&=XY \quad V=Y\\
X&=U/V \quad Y=V\\
J&=1/v\\
f_{U,V}(u,v)&=\dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\dfrac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha+\beta)\Gamma(\gamma)}(u/v)^{\alpha-1}(1-u/v)^{\beta-1}v^{\alpha+\beta-1}(1-v)^{\gamma-1}(1/v)\\
0<&u<v<1\\
f_U(u)&=\dfrac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}u^{\alpha-1}\int_{u}^{1}v^{\beta-1}(1-v)^{\gamma-1}((v-u)/v)^{\beta-1} \ dv\\
&\dfrac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta+\gamma)}u^{\alpha-1}(1-u)^{\beta+\gamma-1} \ 0<u<1\\
U&\sim gamma(\alpha,\beta+\gamma)
\end{align*}
\end{enumerate}
\section*{Problem 5}
\begin{enumerate}[(a)]
\item 
\begin{align*}
Y|X & \sim N(x,x^2)\\
E(Y|X)&=X\\
Var(Y|X)&=X^2\\
X&\sim U(0,1)\\
EY &= E(E(Y|X))=EX\\
f_X(x)&=1 \quad \text{for } 0\leq x \leq 1\\
EX&=\dfrac{1}{2}=EY\\
Var(Y)&=E(Var(Y|X))+Var(E(Y|X))=E(X^2)+Var(X)\\
Var(X)&=\dfrac{1}{12}\\
E(X^2)&=\int_{0}^{1}X^2 \ \dx=1/3\\
Var(Y)&=1/12+1/3=\dfrac{5}{12}\\
Cov(X,Y)&=E(XY)-E(X)E(Y)\\
&=E(XY)-(1/2)^2=E(XY)-1/4\\
E(XY)&=E[E(XY|X)]=E(XE(Y|X))=E(X^2)=1/3\\
Cov(X,Y)&=1/3-1/4=1/12
\end{align*}
\item
\begin{align*}
f(Y|X=x)&=\dfrac{1}{\sqrt{2\pi}x}e^{-\dfrac{(y-x)^2}{2x^2}}\\
\text{Since } X&=1 \text{ we have:}\\
f(Y|X=1)&=\dfrac{1}{\sqrt{2\pi}1}e^{-\dfrac{(y-1)^2}{2*1^2}}\\
&=\dfrac{1}{\sqrt{2\pi}}e^{-y^2/2}\\
\text{Thus }& f(Y|X=x) \sim N(1,1) \text{ and Y is independent of X}\\
\text{By }&\text{theorem 4.3.5 Since X and Y are independent, } U=g(X) \text{ and } V=g(y) \text{ are independent}\\
\text{Thus }&\text{the transformation } Y/X \text{ is indepedent of } X
\end{align*}
\end{enumerate}
\section*{Problem 6}
\begin{align*}
\text{Since the } &X_is \text{ are i.i.d.:}\\
\text{We }&\text{can apply Theorem 4.6.7}\\
M_H(t)&=(M_{\mathbb{X}}(t))^n \\
M_H(t)&= Ee^{Ht}=EE(e^{Ht}|N)=EE(d^{(X_1+\dots+X_N)t}|N)\\
&=E\{[E(e^{X_1t}|N)]^N\}\\
Ee^{X_1t}&=\sum_{x_1=1}^{\infty}e^{x_1t}\dfrac{-(1-p)^{x_1}}{x_1}\\
&=\dfrac{-1}{\log(p)}\sum_{x_1=1}^{\infty}\dfrac{(e^{t}(1-p))^{x_1}}{x_1}\\
&=\dfrac{-1}{\log(p)}(-\log(1-e^{t(1-p)}))\\
&=\dfrac{\log(1-e^t(1-p))}{\log(p)}\\
N&\sim \dfrac{e^{-\lambda} \lambda^n}{n!}\\
E(\dfrac{\log(1-e^t(1-p))}{\log(p)})^N&=\sum_{n=0}^{\infty}(\dfrac{\log(1-e^t(1-p))}{\log(p)})^n\dfrac{e^{-\lambda} \lambda^n}{n!}\\
&=e^{-\lambda}((p-1)e^t+1)^{\dfrac{\lambda}{\log(p)}}\\
&=e^{\log(p)^{-\lambda/\log(p)}}\left(\dfrac{1}{(p-1)e^t+1}\right)^{\dfrac{-\lambda}{\log(p)}}\\
&=\left(\dfrac{p}{1-e^t(1-p)}\right)^{\dfrac{-\lambda}{\log(p)}}\\
\text{Which }& \text{is the mgf of } negbin(-\lambda/\log(p),p)\\
\end{align*}
\pagebreak
\section*{Problem 7}
\chead{Problems 7-9}
\begin{align*}
Cov(X_1+X_2,X_2+X_3)&=E[(X_1+X_2)(X_2+X_3)]-E(X_1+X_2)E(X_2+X_3)\\
E(X_1+X_2)E(X_2+X_3)&=(\mu+\mu)(\mu+\mu)=4\mu^2\\
E[(X_1+X_2)(X_2+X_3)]&=E(X_1X_2+X_1X_3+X_2X_3+X_2^2)\\
&=E(X_1X_2)+E(X_1X_3)E(X_2X_3)+E(X_2^2)\\
&=E(X_1)E(X_2)+E(X_1)E(X_3)+E(X_2)E(X_3)+E(X_2^2)\\
&=E(X_1)E(X_2)+E(X_1)E(X_3)+E(X_2)E(X_3)+E(X_2^2)-E(X_2)^2+E(X_2)^2\\
&=4\mu_2+\sigma^2\\
Cov(X_1+X_2,X_2+X_3)&=4\mu_2+\sigma^2-4\mu^2=\sigma^2\\
Cov(X_1+X_2,X_1-X_2)&=E[(X_1+X_2)(X_1-X_2)]-E(X_1+X_2)E(X_1-X_2)\\
E(X_1+X_2)E(X_1-X_2)&=2\mu*(\mu-\mu)=0\\
E[(X_1+X_2)(X_1-X_2)]&=E(X_1^2-X_2^2)\\
&=E(X_1^2)-E(X_2^2)\\
&=(E(X_1^2)-E(X_1)^2)+E(X_1)^2-E(X_2^2)-E(X_2)^2+E(X_2)^2\\
&=\sigma^2+E(X_1)^2-(E(X_2^2)-E(X_2)^2+E(X_2)^2)\\
&=\sigma^2+\mu^2-\sigma^2-\mu^2=0\\
Cov(X_1+X_2,X_2+X_3)&=0-0=0
\end{align*}
\section*{Problem 8}
\begin{align*}
f(x,y)&=\dfrac{1}{2\pi(1-p^2)^{1/2}}\exp\left(\dfrac{-(x^2-2\rho xy+y^2)}{2(1-\rho^2)}\right)\\
\text{Since}&\text{ this is the standard bivariate normal density:}\\
\mu_x&=0 \ \mu_y=0 \ \sigma^2_x=1 \ \sigma^2_y=1\\
\text{WTS: } Corr(X,Y)&=\rho\\
Corr(X,Y)&=\dfrac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\\
&=\dfrac{E(XY)-E(X)E(Y)}{\sqrt{1*1}}\\
&=E(XY)=E(E(XY|X))\\
&=E(X E(Y|X))\\
\text{Since } Y|X \sim N(\rho X, 1-p^2) \text{ we have:}\\
&=E(\rho X^2)=\rho E(X^2)\\
E(X^2)=1 \text{ since:}\\
E(X^2)&=E(X^2)-E(X)^2+E(X)^2\\
&=Var(X)+E(X)^2=1+0=1\\
\rho E(X^2)&=\rho\\
\text{Thus }& Corr(X,Y)=\rho
\end{align*}
\begin{align*}
\text{WTS: } Corr(X^2,Y^2)&=\rho^2\\
Corr(X^2,Y^2)&=\dfrac{Cov(X^2,Y^2)}{\sqrt{Var(X^2)Var(Y^2)}}\\
Cov(X^2,Y^2)&=E(X^2Y^2)-E(X^2)E(Y^2)\\
\text{Since }& E(Y^2)=E(X^2)=1 \text{ we have:}\\
&=E(X^2Y^2)-1\\
&=E(E(X^2Y^2|X))-1\\
&=E(X^2E(Y^2|X))-1\\
E(Y^2|X)&=E(Y^2|X)-E(Y|X)^2+E(Y|X)^2\\
&=Var(Y|X)+E(Y|X)^2=1-\rho^2+\rho^2 X^2\\
Cov(X^2,Y^2)&=E(X^2(1-\rho^2+\rho^2 X^2))-1\\
&=E(X^2-\rho^2X^2+\rho^2X^4)-1\\
&=1-\rho^2+\rho^2E(X^4)-1\\
&=-\rho^2+\rho^2E(X^4)\\
E(X^4)&=E(X)^4+6E(X)^2Var(X)+3Var(X)^2=3\\
\text{Plugging}&\text{ this in we have:}\\
&Cov(X^2,Y^2)=-\rho^2+3\rho^2=2\rho^2\\
\sqrt{Var(X^2)Var(Y^2)}&=\sqrt{(E(X^4)-E(X^2)^2)(E(Y^4)-E(Y^2)^2)}\\
&=\sqrt{(E(X^4)-1)(E(Y^4)-1)}\\
&=\sqrt{(3-1)(3-1)}=\sqrt{4}=2\\
Corr(X^2,Y^2)&=2\rho^2/2=\rho^2
\end{align*}
\section*{Problem 9}
\begin{enumerate}[(a)]
\item
\begin{align*}
\text{WTS: } Cov(X,Y)&=Cov(X,E(Y|X))\\
Cov(X,Y)&=E(XY)-E(X)E(Y)\\
Cov(X,E(Y|X))&=E(XE(Y|X))-E(X)E(E(Y|X))\\
\text{Since }& E(E(Y|X))=E(Y) \text{ we have:}\\
&=E(XE(Y|X))-E(X)E(Y)\\
E(XE(Y|X))&=E(E(XY|X))=E(XY)\\
\text{Plugging}&\text{ this in we have:}\\
Cov(X,E(Y|X))&=E(XY)-E(X)E(Y)=Cov(X,Y)
\end{align*}
\item
\begin{align*}
\text{WTS: }& Cov(X,Y-E(Y|X))=0\\
Cov(X,Y-E(Y|X))&=E(X(Y-E(Y|X)))-E(X)E(Y-E(Y|X))\\
&=E(XY-XE(Y|X))-E(X)(E(Y)-E(E(Y|X))\\
&=E(XY)-E(XE(Y|X))-E(X)(E(Y)-E(Y))\\
&=E(XY)-E(XE(Y|X))-0\\
E(XE(Y|X))&=E(E(XY|X))=E(XY)\\
\text{Plugging}&\text{ this in we have:}\\
&=E(XY)-E(XY)\\
&=0
\end{align*}
\item
\begin{align*}
\text{WTS: }& Var(Y-E(Y|X))=E(Var(Y|X))\\
Var(Y-E(Y|X))&=Var(Y)+Var(E(Y|X))-2Cov(Y,E(Y|X))\\
Cov(Y,E(Y|X))&=E(YE(Y|X))-E(Y)E(E(Y|X))\\
E(Y|E(Y|X))&=E(E(YE(Y|X)|X))=E(E(Y|X)E(Y|X)\\
&=E((E(Y|X))^2)\\
E(Y)E(E(Y|X))&=(E(E(Y|X)))^2\\
\text{Plugging}&\text{ this in we get:}\\
Cov(Y,E(Y|X))&=E((E(Y|X))^2)-(E(E(Y|X)))^2\\
&=Var(E(Y|X))\\
\text{Putting}&\text{ this back we have:}\\
Var(Y-E(Y|X))&=Var(Y)+Var(E(Y|X))-2Var(E(Y|X))\\
&=Var(Y)-Var(E(Y|X))\\
&=E(Var(Y|X))+Var(E(X|Y))-Var(E(Y|X))\\
&=E(Var(Y|X))
\end{align*}
\end{enumerate}
\end{flushleft}
\end{document}
E(Var(Y|X))&=Var(Y)-Var(E(Y|X))\\